{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import fmpsdk as fmp\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "fmp_key = os.getenv(\"fmp_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = SparkSession.builder \\\n",
    "        .appName(\"crypto-Data\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function historical_price_full in module fmpsdk.general:\n",
      "\n",
      "historical_price_full(apikey: str, symbol: Union[str, List], time_series: int = None, series_type: str = None, from_date: str = None, to_date: str = None) -> Optional[List[Dict]]\n",
      "    Query FMP Historical Price Full API.\n",
      "    \n",
      "    This API endpoint is a multifunction tool!\n",
      "    \n",
      "    :param apikey: Your API Key\n",
      "    :param symbol: The Ticker, Index, Commodity, etc. symbol to query for.\n",
      "    :param time_series: Not sure what this is.  5 is the only value I've seen used.\n",
      "    :param series_type: Not sure what this is.  \"line\" is the only option I've seen used.\n",
      "    :param from_date: 'YYYY-MM-DD' format\n",
      "    :param to_date: 'YYYY-MM-DD' format\n",
      "    :return: A list of dictionaries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fmp.historical_price_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df = fmp.historical_price_full(apikey=fmp_key,symbol=\"BTCUSD\",from_date='2004-02-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.DataFrame(btc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>volume</th>\n",
       "      <th>unadjustedVolume</th>\n",
       "      <th>change</th>\n",
       "      <th>changePercent</th>\n",
       "      <th>vwap</th>\n",
       "      <th>label</th>\n",
       "      <th>changeOverTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-24</td>\n",
       "      <td>50737.0</td>\n",
       "      <td>51717.00</td>\n",
       "      <td>50576.55</td>\n",
       "      <td>51496.02</td>\n",
       "      <td>51468.07031</td>\n",
       "      <td>15118066688</td>\n",
       "      <td>15118066688</td>\n",
       "      <td>759.02</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>51180.27</td>\n",
       "      <td>February 24, 24</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-23</td>\n",
       "      <td>51300.1</td>\n",
       "      <td>51537.00</td>\n",
       "      <td>50227.00</td>\n",
       "      <td>50737.00</td>\n",
       "      <td>50731.94922</td>\n",
       "      <td>21427078270</td>\n",
       "      <td>21427078270</td>\n",
       "      <td>-563.10</td>\n",
       "      <td>-1.10000</td>\n",
       "      <td>51018.44</td>\n",
       "      <td>February 23, 24</td>\n",
       "      <td>-0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>51866.9</td>\n",
       "      <td>52100.00</td>\n",
       "      <td>50890.20</td>\n",
       "      <td>51258.70</td>\n",
       "      <td>51304.97266</td>\n",
       "      <td>25413900611</td>\n",
       "      <td>25413900611</td>\n",
       "      <td>-608.20</td>\n",
       "      <td>-1.17000</td>\n",
       "      <td>51592.97</td>\n",
       "      <td>February 22, 24</td>\n",
       "      <td>-0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>52270.1</td>\n",
       "      <td>52394.91</td>\n",
       "      <td>50439.00</td>\n",
       "      <td>51851.67</td>\n",
       "      <td>51839.17969</td>\n",
       "      <td>28624907020</td>\n",
       "      <td>28624907020</td>\n",
       "      <td>-418.43</td>\n",
       "      <td>-0.80052</td>\n",
       "      <td>51313.06</td>\n",
       "      <td>February 21, 24</td>\n",
       "      <td>-0.008005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>51771.2</td>\n",
       "      <td>53000.00</td>\n",
       "      <td>50584.00</td>\n",
       "      <td>52167.00</td>\n",
       "      <td>52284.87500</td>\n",
       "      <td>33353758256</td>\n",
       "      <td>33353758256</td>\n",
       "      <td>395.80</td>\n",
       "      <td>0.76452</td>\n",
       "      <td>51914.82</td>\n",
       "      <td>February 20, 24</td>\n",
       "      <td>0.007645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     open      high       low     close     adjClose  \\\n",
       "0  2024-02-24  50737.0  51717.00  50576.55  51496.02  51468.07031   \n",
       "1  2024-02-23  51300.1  51537.00  50227.00  50737.00  50731.94922   \n",
       "2  2024-02-22  51866.9  52100.00  50890.20  51258.70  51304.97266   \n",
       "3  2024-02-21  52270.1  52394.91  50439.00  51851.67  51839.17969   \n",
       "4  2024-02-20  51771.2  53000.00  50584.00  52167.00  52284.87500   \n",
       "\n",
       "        volume  unadjustedVolume  change  changePercent      vwap  \\\n",
       "0  15118066688       15118066688  759.02        1.50000  51180.27   \n",
       "1  21427078270       21427078270 -563.10       -1.10000  51018.44   \n",
       "2  25413900611       25413900611 -608.20       -1.17000  51592.97   \n",
       "3  28624907020       28624907020 -418.43       -0.80052  51313.06   \n",
       "4  33353758256       33353758256  395.80        0.76452  51914.82   \n",
       "\n",
       "             label  changeOverTime  \n",
       "0  February 24, 24        0.015000  \n",
       "1  February 23, 24       -0.011000  \n",
       "2  February 22, 24       -0.011700  \n",
       "3  February 21, 24       -0.008005  \n",
       "4  February 20, 24        0.007645  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3759"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "    or a :class:`numpy.ndarray`.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None. The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    \n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "    \n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "        later.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring. The first few rows will be used\n",
      "        if ``samplingRatio`` is ``None``.\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "        .. versionadded:: 2.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create a DataFrame from a list of tuples.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
      "    +-----+---+\n",
      "    |   _1| _2|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a list of dictionaries.\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  1|Alice|\n",
      "    +---+-----+\n",
      "    \n",
      "    Create a DataFrame with column names specified.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the explicit schema specified.\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the schema in DDL formatted string.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create an empty DataFrame.\n",
      "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
      "    as the DataFrame lacks data from which the schema can be inferred.\n",
      "    \n",
      "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    +----+---+\n",
      "    \n",
      "    Create a DataFrame from Row objects.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
      "    >>> df.show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a pandas DataFrame.\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    +---+---+\n",
      "    |  0|  1|\n",
      "    +---+---+\n",
      "    |  1|  2|\n",
      "    +---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark2.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark2.createDataFrame(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------+--------+-----------+-----------+----------------+------+-------------+--------+---------------+--------------+\n",
      "|      date|   open|   high|     low|   close|   adjClose|     volume|unadjustedVolume|change|changePercent|    vwap|          label|changeOverTime|\n",
      "+----------+-------+-------+--------+--------+-----------+-----------+----------------+------+-------------+--------+---------------+--------------+\n",
      "|2024-02-24|50737.0|51717.0|50576.55|51496.02|51468.07031|15118066688|     15118066688|759.02|          1.5|51180.27|February 24, 24|         0.015|\n",
      "|2024-02-23|51300.1|51537.0| 50227.0| 50737.0|50731.94922|21427078270|     21427078270|-563.1|         -1.1|51018.44|February 23, 24|        -0.011|\n",
      "|2024-02-22|51866.9|52100.0| 50890.2| 51258.7|51304.97266|25413900611|     25413900611|-608.2|        -1.17|51592.97|February 22, 24|       -0.0117|\n",
      "+----------+-------+-------+--------+--------+-----------+-----------+----------------+------+-------------+--------+---------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\",\"jdbc:sqlserver://ZAHRA\\SQLEXPRESS:61254;database=stock_fundamentals;trustServerCertificate=true;encrypt=true\") \\\n",
    "    .option(\"dbtable\",\"btc_data\") \\\n",
    "    .option(\"user\",\"mehassan\") \\\n",
    "    .option(\"password\",\"password\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
